{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import raw_taq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'raw_taq' from '/home/rdhyee/dlab-finance/basic-taq/raw_taq.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can run this if you update the raw_taq.py file\n",
    "from importlib import reload\n",
    "reload(raw_taq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = \"../local_data/EQY_US_ALL_BBO_20150102.zip\"\n",
    "taq_file = raw_taq.TAQ2Chunks(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how far can we walk through the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's time just walking through a file vs various chunk size\n",
    "from itertools import islice\n",
    "\n",
    "def walk_through_file(fname, chunk_size=1000, max_chunk=None):\n",
    "    taq_file = raw_taq.TAQ2Chunks(fname)\n",
    "    for chunk in islice(taq_file.convert_taq(chunk_size), max_chunk):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(myenv3)rdhyee@mercury:~/dlab-finance/basic-taq$ time cat ../local_data/EQY_US_ALL_BBO_20150102.zip > /dev/null \n",
    "\n",
    "real\t0m33.261s\n",
    "user\t0m0.013s\n",
    "sys\t0m1.698s\n",
    "```\n",
    "\n",
    "trying gzip on entire file caused error:\n",
    "\n",
    "```\n",
    "time gzip -cdfq ../local_data/EQY_US_ALL_BBO_20150102.zip > /dev/null \n",
    "\n",
    "gzip: ../local_data/EQY_US_ALL_BBO_20150102.zip: invalid compressed data--length error\n",
    "\n",
    "real\t3m40.387s\n",
    "user\t3m39.351s\n",
    "sys\t0m1.000s\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on savio:\n",
    "\n",
    "[ryee@n0045 ~]$ time cat davclark/taq-mirror/EQY_US_ALL_BBO/EQY_US_ALL_BBO_2015/EQY_US_ALL_BBO_201501/EQY_US_ALL_BBO_20150102.zip > /dev/null\n",
    "\n",
    "real\t0m5.851s\n",
    "user\t0m0.005s\n",
    "sys\t0m4.411s\n",
    "\n",
    "```\n",
    "[ryee@n0045 ~]$ time gzip -cdfq davclark/taq-mirror/EQY_US_ALL_BBO/EQY_US_ALL_BBO_2015/EQY_US_ALL_BBO_201501/EQY_US_ALL_BBO_20150102.zip > /dev/null\n",
    "\n",
    "gzip: davclark/taq-mirror/EQY_US_ALL_BBO/EQY_US_ALL_BBO_2015/EQY_US_ALL_BBO_201501/EQY_US_ALL_BBO_20150102.zip: invalid compressed data--length error\n",
    "\n",
    "real\t4m33.470s\n",
    "user\t4m31.036s\n",
    "sys\t0m2.561s\n",
    "\n",
    "```\n",
    "\n",
    "Need to do unzip\n",
    "\n",
    "```\n",
    "[ryee@n0045 ~]$ time unzip -c davclark/taq-mirror/EQY_US_ALL_BBO/EQY_US_ALL_BBO_2015/EQY_US_ALL_BBO_201501/EQY_US_ALL_BBO_20150102.zip > /dev/null\n",
    "\n",
    "real\t4m30.988s\n",
    "user\t4m26.768s\n",
    "sys\t0m4.367s\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time walk_through_file(fname, chunk_size=200000, max_chunk=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000000 records:\n",
    "\n",
    "```\n",
    "10000 chunks = 18.2s\n",
    "1000 x 1000 = 9.5s\n",
    "100 chunks x 10000/chunk 7.32s\n",
    "10 chunks x 100000/chunk 7.37s\n",
    "5 chunks x 2000000/chunk 7.69s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 10,000,000 records\n",
    "\n",
    "%time walk_through_file(fname, chunk_size=100000, max_chunk=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10,000,000 records:\n",
    "\n",
    "10,000 chunks x 1000/chunk 85s\n",
    "5,000 chunks x 2000/chunk 79s\n",
    "100 chunks x 100,000/chunk 76s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "686099151 / 10000000 * 76 / (3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code to walk through a zip file\n",
    "\n",
    "def raw_chunks_from_zipfile(fname, chunksize=1000, BYTES_PER_LINE=98):\n",
    "    import zipfile\n",
    "    import datetime\n",
    "\n",
    "    with zipfile.ZipFile(fname, 'r') as zfile:\n",
    "        for inside_f in zfile.filelist:\n",
    "           \n",
    "            with zfile.open(inside_f.filename) as infile:\n",
    "                first = infile.readline()\n",
    "                \n",
    "                still_bytes = True\n",
    "                while(still_bytes):\n",
    "                    raw_bytes = infile.read(BYTES_PER_LINE * chunksize)\n",
    "                    if raw_bytes:\n",
    "                        yield(still_bytes)\n",
    "                    else:\n",
    "                        still_bytes = False\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def walk_through_zip_raw(fname,chunksize=100000,max_chunk=None):\n",
    "    for (i, chunk) in enumerate(islice(raw_chunks_from_zipfile(fname, chunksize=chunksize),max_chunk)):\n",
    "        pass\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 31s, sys: 1min 5s, total: 3min 36s\n",
      "Wall time: 3min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "686"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time walk_through_zip_raw(fname,chunksize=1000000,max_chunk=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# process by row or by chunk?\n",
    "def taq_row(fname, chunk_size=1000):\n",
    "    taq_file = raw_taq.TAQ2Chunks(fname)\n",
    "    for chunk in taq_file.convert_taq(chunk_size):\n",
    "        for row in chunk:\n",
    "            yield row\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (i,row) in enumerate(islice(taq_row(fname), 1000000)):\n",
    "    print(\"\\r {0}\".format(i), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row.converted_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row.dtype.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you want just the type\n",
    "row.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for field in row.dtype.names:\n",
    "    print (field, row[field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# converting im\n",
    "import datetime\n",
    "datetime.datetime.fromtimestamp(1420230800.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accumulate (exchange, symbol_root, symbol_suffix)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "exchanges = Counter()\n",
    "symbol_roots = Counter()\n",
    "exchange_symbol_root_suffixes = Counter()\n",
    "\n",
    "for (i,row) in enumerate(islice(taq_row(fname, chunk_size=1000000), 10000000)):\n",
    "    exchange = row['Exchange'].decode(\"utf-8\", \"strict\")\n",
    "    symbol_root = row['Symbol_root'].decode(\"utf-8\", \"strict\").strip()\n",
    "    symbol_suffix = row['Symbol_suffix'].decode(\"utf-8\", \"strict\").strip()\n",
    "    \n",
    "    triplet = (exchange, symbol_root, symbol_suffix)\n",
    "    \n",
    "    print(\"\\r {0}\".format(i),end=\"\")\n",
    "    exchanges.update([exchange])\n",
    "    symbol_roots.update([symbol_root])\n",
    "    \n",
    "    exchange_symbol_root_suffixes.update([triplet])\n",
    "    \n",
    "exchanges, symbol_roots, exchange_symbol_root_suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(row['Exchange'].decode(\"utf-8\", \"strict\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can also easily convert numpy record arrays to pandas dataframes easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_df = pd.DataFrame(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note that time is not correctly parsed yet:\n",
    "chunk_df.Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Compute some summary statistics across a few securities in the TAQ file\n",
    "\n",
    "Processing an entire TAQ file will take a long time. So, maybe just run through the chunks for the first two securities (you can then exit out of a loop once you see the third security / symbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statistics import mode\n",
    "\n",
    "#find the max bid price\n",
    "max_price = max(chunk['Bid_Price'])\n",
    "\n",
    "#find the min bid price\n",
    "min_price = min(chunk['Bid_Price'])\n",
    "\n",
    "#find the mean of bid price\n",
    "avg_price = np.mean(chunk['Bid_Price'])\n",
    "\n",
    "#find the mod of bid price\n",
    "mod_price = mode(chunk['Bid_Price'])\n",
    "\n",
    "#find the sd of bid price\n",
    "sd_price = np.std(chunk['Bid_Price'])\n",
    "\n",
    "print(\" Max bid price: \", max_price, \"\\n\", \"Min bid price: \", min_price, \"\\n\", \n",
    "      \"Mean bid price: \", avg_price, \"\\n\", \"Mod bid price: \", mod_price, \"\\n\", \"Standard deviation bid price: \", sd_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find the max Ask price\n",
    "max_price = max(chunk['Ask_Price'])\n",
    "\n",
    "#find the min Ask price\n",
    "min_price = min(chunk['Ask_Price'])\n",
    "\n",
    "#find the mean of Ask price\n",
    "avg_price = np.mean(chunk['Ask_Price'])\n",
    "\n",
    "#find the mod of Ask price\n",
    "mod_price = mode(chunk['Ask_Price'])\n",
    "\n",
    "#find the sd of Ask price\n",
    "sd_price = np.std(chunk['Ask_Price'])\n",
    "\n",
    "print(\" Max Ask price: \", max_price, \"\\n\", \"Min Ask price: \", min_price, \"\\n\", \n",
    "      \"Mean Ask price: \", avg_price, \"\\n\", \"Mod Ask price: \", mod_price, \"\\n\", \"Standard deviation Ask price: \", sd_price)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
