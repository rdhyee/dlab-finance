{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "import raw_taq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can run this if you update the raw_taq.py file\n",
    "from importlib import reload\n",
    "reload(raw_taq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = \"../local_data/EQY_US_ALL_BBO_20150102.zip\"\n",
    "taq_file = raw_taq.TAQ2Chunks(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how far can we walk through the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's time just walking through a file vs various chunk size\n",
    "from itertools import islice\n",
    "\n",
    "def walk_through_file(fname, chunk_size=1000, max_chunk=None):\n",
    "    taq_file = raw_taq.TAQ2Chunks(fname)\n",
    "    for chunk in islice(taq_file.convert_taq(chunk_size), max_chunk):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(myenv3)rdhyee@mercury:~/dlab-finance/basic-taq$ time cat ../local_data/EQY_US_ALL_BBO_20150102.zip > /dev/null \n",
    "\n",
    "real\t0m33.261s\n",
    "user\t0m0.013s\n",
    "sys\t0m1.698s\n",
    "```\n",
    "\n",
    "trying gzip on entire file caused error:\n",
    "\n",
    "```\n",
    "time gzip -cdfq ../local_data/EQY_US_ALL_BBO_20150102.zip > /dev/null \n",
    "\n",
    "gzip: ../local_data/EQY_US_ALL_BBO_20150102.zip: invalid compressed data--length error\n",
    "\n",
    "real\t3m40.387s\n",
    "user\t3m39.351s\n",
    "sys\t0m1.000s\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on savio:\n",
    "\n",
    "[ryee@n0045 ~]$ time cat davclark/taq-mirror/EQY_US_ALL_BBO/EQY_US_ALL_BBO_2015/EQY_US_ALL_BBO_201501/EQY_US_ALL_BBO_20150102.zip > /dev/null\n",
    "\n",
    "real\t0m5.851s\n",
    "user\t0m0.005s\n",
    "sys\t0m4.411s\n",
    "\n",
    "```\n",
    "[ryee@n0045 ~]$ time gzip -cdfq davclark/taq-mirror/EQY_US_ALL_BBO/EQY_US_ALL_BBO_2015/EQY_US_ALL_BBO_201501/EQY_US_ALL_BBO_20150102.zip > /dev/null\n",
    "\n",
    "gzip: davclark/taq-mirror/EQY_US_ALL_BBO/EQY_US_ALL_BBO_2015/EQY_US_ALL_BBO_201501/EQY_US_ALL_BBO_20150102.zip: invalid compressed data--length error\n",
    "\n",
    "real\t4m33.470s\n",
    "user\t4m31.036s\n",
    "sys\t0m2.561s\n",
    "\n",
    "```\n",
    "\n",
    "Need to do unzip\n",
    "\n",
    "```\n",
    "[ryee@n0045 ~]$ time unzip -c davclark/taq-mirror/EQY_US_ALL_BBO/EQY_US_ALL_BBO_2015/EQY_US_ALL_BBO_201501/EQY_US_ALL_BBO_20150102.zip > /dev/null\n",
    "\n",
    "real\t4m30.988s\n",
    "user\t4m26.768s\n",
    "sys\t0m4.367s\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time walk_through_file(fname, chunk_size=200000, max_chunk=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000000 records:\n",
    "\n",
    "```\n",
    "10000 chunks = 18.2s\n",
    "1000 x 1000 = 9.5s\n",
    "100 chunks x 10000/chunk 7.32s\n",
    "10 chunks x 100000/chunk 7.37s\n",
    "5 chunks x 2000000/chunk 7.69s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 10,000,000 records\n",
    "\n",
    "%time walk_through_file(fname, chunk_size=100000, max_chunk=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10,000,000 records:\n",
    "\n",
    "10,000 chunks x 1000/chunk 85s\n",
    "5,000 chunks x 2000/chunk 79s\n",
    "100 chunks x 100,000/chunk 76s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "686099151 / 10000000 * 76 / (3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code to walk through a zip file\n",
    "\n",
    "# function to calculate mapping of line len to index of last column\n",
    "\n",
    "def record_len_to_last_column(initial_dtype):\n",
    "    \"\"\"\n",
    "    initial_dtype of form:\n",
    "    \n",
    "    [('Time', 9),\n",
    " ('Exchange', 1),\n",
    " ('Symbol_root', 6),\n",
    " ('Symbol_suffix', 10),\n",
    " ('Bid_Price', 11),\n",
    " ('Bid_Size', 7),\n",
    " ('Ask_Price', 11),\n",
    " ....\n",
    " ('newline', 'S2')]\n",
    " \n",
    " Assumption is that the last field is a newline field that is present in all versions of BBO\n",
    "    \"\"\"\n",
    "    \n",
    "    cum_len = 0\n",
    "    cum_lens = []\n",
    "    flens = [(field, int(dtype[1:])) for (field, dtype) in raw_taq.initial_dtype]\n",
    "    newline_len = flens[-1][1]\n",
    "\n",
    "    for (i,(field, flen)) in enumerate(flens[:-1]):\n",
    "        cum_len += flen\n",
    "        cum_lens.append((cum_len+newline_len, i))\n",
    "\n",
    "    return dict(cum_lens)\n",
    "    \n",
    "    \n",
    "\n",
    "def raw_chunks_from_zipfile(fname, chunksize=1000):\n",
    "    import zipfile\n",
    "    import datetime\n",
    "\n",
    "    with zipfile.ZipFile(fname, 'r') as zfile:\n",
    "        for inside_f in zfile.filelist:\n",
    "           \n",
    "            # can I do two passes -- first pass is to read 2 first two lines \n",
    "            \n",
    "            with zfile.open(inside_f.filename) as infile:\n",
    "                first = infile.readline()  # we can process first line\n",
    "                second = infile.readline()\n",
    "                bytes_per_line = len(second)\n",
    "        \n",
    "            with zfile.open(inside_f.filename) as infile:\n",
    "                first = infile.readline()\n",
    "                \n",
    "                still_bytes = True\n",
    "                while (still_bytes):\n",
    "                    raw_bytes = infile.read(bytes_per_line * chunksize)\n",
    "                    if raw_bytes:\n",
    "                        yield (raw_bytes)\n",
    "                    else:\n",
    "                        still_bytes = False\n",
    "\n",
    "RECORD_LEN_TO_LAST_COLUMN_MAP = record_len_to_last_column(raw_taq.initial_dtype)                \n",
    "\n",
    "def chunks_from_zipfile(fname, chunksize=1000):\n",
    "    import zipfile\n",
    "    import datetime\n",
    "    \n",
    "    \n",
    "    with zipfile.ZipFile(fname, 'r') as zfile:\n",
    "        for inside_f in zfile.filelist:\n",
    "                   \n",
    "            with zfile.open(inside_f.filename) as infile:\n",
    "                first = infile.readline()\n",
    "                bytes_per_line = len(first)\n",
    "                dtype = raw_taq.initial_dtype[:RECORD_LEN_TO_LAST_COLUMN_MAP[bytes_per_line]] + \\\n",
    "                   [raw_taq.initial_dtype[-1]]\n",
    "                    \n",
    "                more_bytes = True\n",
    "                \n",
    "                while (more_bytes):\n",
    "                    raw_bytes = infile.read(bytes_per_line * chunksize)\n",
    "                    all_strings = np.ndarray(len(raw_bytes) // bytes_per_line, \n",
    "                                             buffer=raw_bytes, dtype=dtype)\n",
    "                    \n",
    "                    if raw_bytes:\n",
    "                        yield (all_strings)\n",
    "                    else:\n",
    "                        more_bytes = False    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def walk_through_zip_raw(fname,chunksize=100000,max_chunk=None):\n",
    "    for (i, chunk) in enumerate(islice(raw_chunks_from_zipfile(fname, chunksize=chunksize),max_chunk)):\n",
    "        pass\n",
    "    return i\n",
    "\n",
    "def walk_through_zip_init_conv(fname,chunksize=100000,max_chunk=None):\n",
    "    LINE_WIDTH = 98 # will have to generalize to get line size out\n",
    "    expected_buffer_size = chunksize *  LINE_WIDTH\n",
    "    \n",
    "    for (i, chunk) in enumerate(islice(raw_chunks_from_zipfile(fname, chunksize=chunksize),max_chunk)):\n",
    "        try:\n",
    "            all_strings = np.ndarray(chunksize, buffer=chunk, dtype=raw_taq.initial_dtype)\n",
    "        except Exception as e:\n",
    "            all_strings = np.ndarray(len(chunk) // LINE_WIDTH, buffer=chunk, dtype=raw_taq.initial_dtype)\n",
    "            \n",
    "    return i\n",
    "            \n",
    "    \n",
    "def walk_through_zip_init_conv_0(fname,chunksize=100000,max_chunk=None):\n",
    "    \n",
    "    for (i, chunk) in enumerate(islice(raw_chunks_from_zipfile(fname, chunksize=chunksize),max_chunk)):\n",
    "        all_strings = np.ndarray(chunksize, buffer=chunk, dtype=raw_taq.initial_dtype)\n",
    "            \n",
    "    return i\n",
    "                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time walk_through_zip_raw(fname,chunksize=1000000,max_chunk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time walk_through_zip_init_conv(fname,chunksize=1000000,max_chunk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time walk_through_zip_init_conv_0(fname,chunksize=1000000,max_chunk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's play with parsing chunks\n",
    "\n",
    "chunk = next(chunks_from_zipfile(fname,chunksize=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://docs.scipy.org/doc/numpy/reference/generated/numpy.fromstring.html\n",
    "\n",
    "np.ndarray(1, buffer=chunk, dtype=raw_taq.initial_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# process by row or by chunk?\n",
    "def taq_row(fname, chunk_size=1000):\n",
    "    taq_file = raw_taq.TAQ2Chunks(fname)\n",
    "    for chunk in taq_file.convert_taq(chunk_size):\n",
    "        for row in chunk:\n",
    "            yield row\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (i,row) in enumerate(islice(taq_row(fname), 1000000)):\n",
    "    print(\"\\r {0}\".format(i), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row.converted_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row.dtype.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you want just the type\n",
    "row.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for field in row.dtype.names:\n",
    "    print (field, row[field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# converting im\n",
    "import datetime\n",
    "datetime.datetime.fromtimestamp(1420230800.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "counts = np.unique(chunk[:]['Symbol_root'], return_counts=True)\n",
    "c.update(dict(zip_longest(counts[0], counts[1])))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accumulate (exchange, symbol_root, symbol_suffix)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def count_chunk_elements(fname, chunksize=1000000, max_chunk=None):\n",
    "\n",
    "    symbol_roots = Counter()\n",
    "\n",
    "    for (i,chunk) in enumerate(islice(chunks_from_zipfile(fname, chunksize), max_chunk)):\n",
    "\n",
    "        counts = np.unique(chunk[:]['Symbol_root'], return_counts=True)\n",
    "        symbol_roots.update(dict(zip_longest(counts[0], counts[1])))\n",
    "\n",
    "        print(\"\\r {0}\".format(i),end=\"\")\n",
    "\n",
    "    return symbol_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#faqname = \"../local_data/EQY_US_ALL_BBO_20150102.zip\"\n",
    "faqname = \"../local_data/EQY_US_ALL_BBO_20100104.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time c = count_chunk_elements(fname, max_chunk=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(c.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (i,(k,v)) in enumerate(islice(c.most_common(),10)):\n",
    "    print (\"\\t\".join([str(i), k.decode('utf-8').strip(), str(v)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can also easily convert numpy record arrays to pandas dataframes easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_df = pd.DataFrame(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note that time is not correctly parsed yet:\n",
    "chunk_df.Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Compute some summary statistics across a few securities in the TAQ file\n",
    "\n",
    "Processing an entire TAQ file will take a long time. So, maybe just run through the chunks for the first two securities (you can then exit out of a loop once you see the third security / symbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statistics import mode\n",
    "\n",
    "#find the max bid price\n",
    "max_price = max(chunk['Bid_Price'])\n",
    "\n",
    "#find the min bid price\n",
    "min_price = min(chunk['Bid_Price'])\n",
    "\n",
    "#find the mean of bid price\n",
    "avg_price = np.mean(chunk['Bid_Price'])\n",
    "\n",
    "#find the mod of bid price\n",
    "mod_price = mode(chunk['Bid_Price'])\n",
    "\n",
    "#find the sd of bid price\n",
    "sd_price = np.std(chunk['Bid_Price'])\n",
    "\n",
    "print(\" Max bid price: \", max_price, \"\\n\", \"Min bid price: \", min_price, \"\\n\", \n",
    "      \"Mean bid price: \", avg_price, \"\\n\", \"Mod bid price: \", mod_price, \"\\n\", \"Standard deviation bid price: \", sd_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find the max Ask price\n",
    "max_price = max(chunk['Ask_Price'])\n",
    "\n",
    "#find the min Ask price\n",
    "min_price = min(chunk['Ask_Price'])\n",
    "\n",
    "#find the mean of Ask price\n",
    "avg_price = np.mean(chunk['Ask_Price'])\n",
    "\n",
    "#find the mod of Ask price\n",
    "mod_price = mode(chunk['Ask_Price'])\n",
    "\n",
    "#find the sd of Ask price\n",
    "sd_price = np.std(chunk['Ask_Price'])\n",
    "\n",
    "print(\" Max Ask price: \", max_price, \"\\n\", \"Min Ask price: \", min_price, \"\\n\", \n",
    "      \"Mean Ask price: \", avg_price, \"\\n\", \"Mod Ask price: \", mod_price, \"\\n\", \"Standard deviation Ask price: \", sd_price)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
