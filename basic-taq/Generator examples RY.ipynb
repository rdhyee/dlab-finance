{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from itertools import (islice, zip_longest)\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import raw_taq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'raw_taq' from '/home/rdhyee/dlab-finance/basic-taq/raw_taq.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can run this if you update the raw_taq.py file\n",
    "from importlib import reload\n",
    "reload(raw_taq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = \"../local_data/EQY_US_ALL_BBO_20150102.zip\"\n",
    "taq_file = raw_taq.TAQ2Chunks(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code to walk through a zip file\n",
    "\n",
    "# function to calculate mapping of line len to index of last column\n",
    "\n",
    "def record_len_to_last_column(initial_dtype):\n",
    "    \"\"\"\n",
    "    initial_dtype of form:\n",
    "    \n",
    "    [('Time', 'S9'),\n",
    " ('Exchange', 'S1'),\n",
    " ....\n",
    " ('newline', 'S2')]\n",
    " \n",
    " Assumption is that the last field is a newline field that is present in all versions of BBO\n",
    "    \"\"\"\n",
    "    \n",
    "    cum_len = 0\n",
    "    cum_lens = []\n",
    "    flens = [(field, int(dtype[1:])) for (field, dtype) in raw_taq.initial_dtype]\n",
    "    newline_len = flens[-1][1]\n",
    "\n",
    "    for (i,(field, flen)) in enumerate(flens[:-1]):\n",
    "        cum_len += flen\n",
    "        cum_lens.append((cum_len+newline_len, i))\n",
    "\n",
    "    return dict(cum_lens)\n",
    "    \n",
    "    \n",
    "\n",
    "def raw_chunks_from_zipfile(fname, chunksize=1000):\n",
    "    import zipfile\n",
    "    import datetime\n",
    "\n",
    "    with zipfile.ZipFile(fname, 'r') as zfile:\n",
    "        for inside_f in zfile.filelist:\n",
    "           \n",
    "            # can I do two passes -- first pass is to read 2 first two lines \n",
    "            \n",
    "            with zfile.open(inside_f.filename) as infile:\n",
    "                first = infile.readline()  # we can process first line\n",
    "                second = infile.readline()\n",
    "                bytes_per_line = len(second)\n",
    "        \n",
    "            with zfile.open(inside_f.filename) as infile:\n",
    "                first = infile.readline()\n",
    "                \n",
    "                still_bytes = True\n",
    "                while (still_bytes):\n",
    "                    raw_bytes = infile.read(bytes_per_line * chunksize)\n",
    "                    if raw_bytes:\n",
    "                        yield (raw_bytes)\n",
    "                    else:\n",
    "                        still_bytes = False\n",
    "\n",
    "RECORD_LEN_TO_LAST_COLUMN_MAP = record_len_to_last_column(raw_taq.initial_dtype)                \n",
    "\n",
    "\n",
    "def chunks_from_zipfile(fname, chunksize=1000):\n",
    "    import zipfile\n",
    "    import datetime\n",
    "    \n",
    "    \n",
    "    with zipfile.ZipFile(fname, 'r') as zfile:\n",
    "        for inside_f in zfile.filelist:\n",
    "                   \n",
    "            with zfile.open(inside_f.filename) as infile:\n",
    "                first = infile.readline()\n",
    "                bytes_per_line = len(first)\n",
    "                \n",
    "                dtype = raw_taq.initial_dtype[:RECORD_LEN_TO_LAST_COLUMN_MAP[bytes_per_line]+1] + \\\n",
    "                   [raw_taq.initial_dtype[-1]]\n",
    "                    \n",
    "                more_bytes = True\n",
    "                \n",
    "                while (more_bytes):\n",
    "                    raw_bytes = infile.read(bytes_per_line * chunksize)\n",
    "                    all_strings = np.ndarray(len(raw_bytes) // bytes_per_line, \n",
    "                                             buffer=raw_bytes, dtype=dtype)\n",
    "                    \n",
    "                    if raw_bytes:\n",
    "                        yield (all_strings)\n",
    "                    else:\n",
    "                        more_bytes = False    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def walk_through_zip_raw(fname,chunksize=100000,max_chunk=None):\n",
    "    for (i, chunk) in enumerate(islice(raw_chunks_from_zipfile(fname, chunksize=chunksize),max_chunk)):\n",
    "        pass\n",
    "    return i\n",
    "\n",
    "def walk_through_zip_init_conv(fname,chunksize=100000,max_chunk=None):\n",
    "    LINE_WIDTH = 98 # will have to generalize to get line size out\n",
    "    expected_buffer_size = chunksize *  LINE_WIDTH\n",
    "    \n",
    "    for (i, chunk) in enumerate(islice(raw_chunks_from_zipfile(fname, chunksize=chunksize),max_chunk)):\n",
    "        try:\n",
    "            all_strings = np.ndarray(chunksize, buffer=chunk, dtype=raw_taq.initial_dtype)\n",
    "        except Exception as e:\n",
    "            all_strings = np.ndarray(len(chunk) // LINE_WIDTH, buffer=chunk, dtype=raw_taq.initial_dtype)\n",
    "            \n",
    "    return i\n",
    "            \n",
    "    \n",
    "def walk_through_zip_init_conv_0(fname,chunksize=100000,max_chunk=None):\n",
    "    \n",
    "    for (i, chunk) in enumerate(islice(raw_chunks_from_zipfile(fname, chunksize=chunksize),max_chunk)):\n",
    "        all_strings = np.ndarray(chunksize, buffer=chunk, dtype=raw_taq.initial_dtype)\n",
    "            \n",
    "    return i\n",
    "                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# converting im\n",
    "import datetime\n",
    "datetime.datetime.fromtimestamp(1420230800.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accumulate (exchange, symbol_root, symbol_suffix)\n",
    "\n",
    "\n",
    "def count_chunk_elements(fname, chunksize=1000000, max_chunk=None):\n",
    "\n",
    "    symbol_roots = Counter()\n",
    "\n",
    "    for (i,chunk) in enumerate(islice(chunks_from_zipfile(fname, chunksize), max_chunk)):\n",
    "\n",
    "        counts = np.unique(chunk[:]['Symbol_root'], return_counts=True)\n",
    "        symbol_roots.update(dict(zip_longest(counts[0], counts[1])))\n",
    "\n",
    "        print(\"\\r {0}\".format(i),end=\"\")\n",
    "\n",
    "    return symbol_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fame = \"../local_data/EQY_US_ALL_BBO_20150102.zip\"\n",
    "fname = \"../local_data/EQY_US_ALL_BBO_20100104.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 325CPU times: user 1min 44s, sys: 13.8 s, total: 1min 57s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%time c = count_chunk_elements(fname, max_chunk=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325268422"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(c.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tSPY\t10234301\n",
      "1\tQQQ\t6371754\n",
      "2\tVXX\t6180076\n",
      "3\tIWM\t4076960\n",
      "4\tXLE\t3995591\n",
      "5\tXIV\t3337509\n",
      "6\tDIA\t3216624\n",
      "7\tQLD\t3071839\n",
      "8\tAAPL\t2878822\n",
      "9\tUPRO\t2744896\n"
     ]
    }
   ],
   "source": [
    "for (i,(k,v)) in enumerate(islice(c.most_common(),10)):\n",
    "    print (\"\\t\".join([str(i), k.decode('utf-8').strip(), str(v)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try new TAQ2Chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from itertools import (islice, zip_longest)\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import raw_taq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import raw_taq\n",
    "#fname = \"../local_data/EQY_US_ALL_BBO_20150102.zip\"\n",
    "fname = \"../local_data/EQY_US_ALL_BBO_20100104.zip\"\n",
    "chunks = raw_taq.TAQ2Chunks(fname,chunksize=1, process_chunk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "\n",
    "def count_chunk_elements1(fname, chunksize=1000000, max_chunk=None, process_chunk=False):\n",
    "\n",
    "    symbol_roots = Counter()\n",
    "\n",
    "    for (i,chunk) in enumerate(islice(raw_taq.TAQ2Chunks(fname, \n",
    "                                                         chunksize=chunksize, \n",
    "                                                         process_chunk=process_chunk), max_chunk)):\n",
    "\n",
    "        counts = np.unique(chunk[:]['Symbol_root'], return_counts=True)\n",
    "        symbol_roots.update(dict(zip_longest(counts[0], counts[1])))\n",
    "\n",
    "        print(\"\\r {0}\".format(i),end=\"\")\n",
    "\n",
    "    return symbol_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 325CPU times: user 1min 45s, sys: 14.8 s, total: 2min\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%time c = count_chunk_elements1(fname, max_chunk=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325268422"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(c.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tPEF\t3693841\n",
      "1\tUPRO\t3096180\n",
      "2\tUSO\t1742436\n",
      "3\tC\t1642334\n",
      "4\tQQQQ\t1542761\n",
      "5\tBAC\t1504047\n",
      "6\tGLD\t1499900\n",
      "7\tIWM\t1448921\n",
      "8\tSPY\t1392723\n",
      "9\tGDX\t1332082\n"
     ]
    }
   ],
   "source": [
    "for (i,(k,v)) in enumerate(islice(c.most_common(),10)):\n",
    "    print (\"\\t\".join([str(i), k.decode('utf-8').strip(), str(v)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
